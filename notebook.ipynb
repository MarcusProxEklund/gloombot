{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gloomhaven: Jaws of the Lion chatbot**\n",
    "\n",
    "A RAG chatbot for the boardgame Gloomhaven: Jaws of the Lion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text from a PDF file\n",
    "$$\\text{(!!!!!Should be in separate .py file!!!!!)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page in mypdf:\n",
    "        # Extract text from the current page and add spacing\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    # Return the extracted text, stripped of leading/trailing whitespace\n",
    "    return all_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most of this document consists of a Glossary of game terms \n",
      "in alphabetical order for ease of reference (p. 2-21).  \n",
      "The last section contains six appendices: \n",
      "Component Integration (p. 30):  \n",
      "Explains what components  \n",
      "of this game can be used in the  \n",
      "larger Gloomhaven game. \n",
      "Components List (p. 28):  \n",
      "An accounting of all the \n",
      "components in the box.  \n",
      "Treasure Index (p. 31)\n",
      " \n",
      "An index referenced any time \n",
      "a numbered treasure tile in a \n",
      "scenario is looted. \n",
      "“Misplaced Goods” Index (p. 31):  \n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"data/gloomhaven_jotl_glossary.pdf\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Print the first 500 characters of the extracted text\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading environment variables and initializing OpenAI API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sentence-Level Embeddings\n",
    "$$\\text{(!!!!!Should be in separate .py file!!!!!)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using OpenAI.\n",
    "\n",
    "    Args:\n",
    "    text (str): Input text.\n",
    "    model (str): Embedding model name.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(model=model, input=text)\n",
    "    return np.array(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 686 sentence embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Splitting text into sentences (basic split)\n",
    "sentences = extracted_text.split(\". \")\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"Generated {len(embeddings)} sentence embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Similarity Differences\n",
    "$$\\text{(!!!!!Should be in separate .py file!!!!!)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): First vector.\n",
    "    vec2 (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity between consecutive sentences\n",
    "similarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Semantic Chunking\n",
    "$$\\text{(!!!!!Should be in separate .py file!!!!!)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    \"\"\"\n",
    "    Computes chunking breakpoints based on similarity drops.\n",
    "\n",
    "    Args:\n",
    "    similarities (List[float]): List of similarity scores between sentences.\n",
    "    method (str): 'percentile', 'standard_deviation', or 'interquartile'.\n",
    "    threshold (float): Threshold value (percentile for 'percentile', std devs for 'standard_deviation').\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices where chunk splits should occur.\n",
    "    \"\"\"\n",
    "    # Determine the threshold value based on the selected method\n",
    "    if method == \"percentile\":\n",
    "        # Calculate the Xth percentile of the similarity scores\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "    elif method == \"standard_deviation\":\n",
    "        # Calculate the mean and standard deviation of the similarity scores\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        # Set the threshold value to mean minus X standard deviations\n",
    "        threshold_value = mean - (threshold * std_dev)\n",
    "    elif method == \"interquartile\":\n",
    "        # Calculate the first and third quartiles (Q1 and Q3)\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        # Set the threshold value using the IQR rule for outliers\n",
    "        threshold_value = q1 - 1.5 * (q3 - q1)\n",
    "    else:\n",
    "        # Raise an error if an invalid method is provided\n",
    "        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "\n",
    "    # Identify indices where similarity drops below the threshold value\n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute breakpoints using the percentile method with a threshold of 90\n",
    "breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text Into Semantic Chunks\n",
    "$$\\text{(!!!!!Should be in separate .py file!!!!!)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(sentences, breakpoints):\n",
    "    \"\"\"\n",
    "    Splits sentences into semantic chunks.\n",
    "\n",
    "    Args:\n",
    "    sentences (List[str]): List of sentences.\n",
    "    breakpoints (List[int]): Indices where chunking should occur.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    start = 0  # Initialize the start index\n",
    "\n",
    "    # Iterate through each breakpoint to create chunks\n",
    "    for bp in breakpoints:\n",
    "        # Append the chunk of sentences from start to the current breakpoint\n",
    "        chunks.append(\". \".join(sentences[start:bp + 1]) + \".\")\n",
    "        start = bp + 1  # Update the start index to the next sentence after the breakpoint\n",
    "\n",
    "    # Append the remaining sentences as the last chunk\n",
    "    chunks.append(\". \".join(sentences[start:]))\n",
    "    return chunks  # Return the list of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic chunks: 617\n",
      "\n",
      "First text chunk:\n",
      "Most of this document consists of a Glossary of game terms \n",
      "in alphabetical order for ease of reference (p.\n"
     ]
    }
   ],
   "source": [
    "# Create chunks using the split_into_chunks function\n",
    "text_chunks = split_into_chunks(sentences, breakpoints)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(f\"Number of semantic chunks: {len(text_chunks)}\")\n",
    "\n",
    "# Print the first chunk to verify the result\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings for Semantic Chunks\n",
    "$$\\text{(!!!!!Should be in separate .py file!!!!!)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text_chunks):\n",
    "    \"\"\"\n",
    "    Creates embeddings for each text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    # Generate embeddings for each text chunk using the get_embedding function\n",
    "    return [get_embedding(chunk) for chunk in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunk embeddings using the create_embeddings function\n",
    "chunk_embeddings = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Search\n",
    "$$\\text{(!!!!!Should be in separate .py file!!!!!)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for a query.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n",
    "    k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Top-k relevant chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What determines the turn order in Gloomhaven?\n",
      "Context 1:\n",
      " \n",
      "Two figures cannot occupy the same hex.\n",
      " Order of Round\n",
      "Every round of a scenario follows the same order: \n",
      "Card Selection\n",
      "Ordering of Initiative\n",
      "Character and Monster Turns\n",
      "End of Round\n",
      " Ordering of Initiative\n",
      "Ordering of Initiative is the second part of every round.\n",
      "========================================\n",
      "Context 2:\n",
      "\n",
      "Unless the character is exhausted or declares a long \n",
      "rest, two cards are selected at the start of every round, \n",
      "with one chosen to be the initiative card. When it is a \n",
      "character’s turn, based on the initiative value of their \n",
      "initiative card, both cards are played, one after the other \n",
      "in either order, with one being used for its top action and \n",
      "the other being used for its bottom action.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('data/validation_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Get top 2 relevant chunks\n",
    "top_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=2)\n",
    "\n",
    "# Print the query\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Print the top 2 most relevant text chunks\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i+1}:\\n{chunk}\\n{'='*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a response based on retrieved chunks\n",
    "$$\\text{(!!!!!Should be in separate .py file!!!!!)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"gpt-4o-mini-2024-07-18\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The turn order in Gloomhaven is determined by the initiative value of the initiative card selected at the start of the round. Each character selects two cards, one of which is chosen as the initiative card, and the order of play is based on the initiative values of these cards.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
